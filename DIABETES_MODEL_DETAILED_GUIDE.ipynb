{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efb8786a",
   "metadata": {},
   "source": [
    "\n",
    "# Diabetes Risk Modeling — Ultra-Detailed Guide (Step-by-Step, Line-by-Line)\n",
    "\n",
    "**Audience:** Anyone with basic Python and little/no ML background.  \n",
    "**Goal:** Explain every step, why we do it, and how the algorithms work — so you can defend design choices and answer questions.\n",
    "\n",
    "---\n",
    "\n",
    "## 0) What this project does (plain English)\n",
    "\n",
    "We build a **binary classifier** that predicts whether a person is likely to have diabetes (`diabetes_label ∈ {0,1}`) using common lab/clinical features (e.g., age, gender, BMI, fasting glucose, OGTT values, insulin, c‑peptide).  \n",
    "We split the data into **Train / Validation / Test**, engineer extra clinically meaningful features, train multiple models, **tune a threshold** on the validation set, and finally **report metrics** on the test set.\n",
    "\n",
    "You can use it with:  \n",
    "- A **CSV** (e.g., `diabetes_sample_5000.csv`)  \n",
    "- A **Data Warehouse** (BigQuery/Snowflake/SQL Server/etc.; sample connectors included below)  \n",
    "- **Local CPU** or **External GPU** (XGBoost/LightGBM GPU tips included below)\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Data: what the columns mean\n",
    "\n",
    "- `age` — Age in years (float or int)  \n",
    "- `gender` — `\"Male\"` or `\"Female\"` (string/categorical)  \n",
    "- `bmi` — Body Mass Index (kg/m²), float  \n",
    "- `glucose_fasting` — Fasting Plasma Glucose (mg/dL), float  \n",
    "- `insulin_fasting` — Fasting insulin (µU/mL), float  \n",
    "- `c_peptide_fasting` — Fasting C‑peptide (ng/mL), float  \n",
    "- `ogtt_1h_glucose` — 1‑hour OGTT glucose (mg/dL), float  \n",
    "- `ogtt_2h_glucose` — 2‑hour OGTT glucose (mg/dL), float  \n",
    "- `diabetes_label` — Target (1 = diabetes, 0 = no diabetes)\n",
    "\n",
    "> **Note**: The model assumes glucose units are **mg/dL** (standard US units). If your source is mmol/L, you’ll need to convert or adapt formulas (e.g., HOMA-IR).\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Loading the CSV and quick data quality checks\n",
    "\n",
    "**Why:** Before modeling, we confirm the file loads, no missing critical columns, and we summarize missing values/duplicates/outliers.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "csv_path = \"diabetes_sample_5000.csv\"  # <- your CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Head:\\n\", df.head(3))\n",
    "\n",
    "# Basic quality\n",
    "print(\"Missing values:\\n\", df.isna().sum())\n",
    "print(\"Duplicate rows:\", df.duplicated().sum())\n",
    "```\n",
    "\n",
    "**What happens:**  \n",
    "- `pd.read_csv` reads file into `df`.  \n",
    "- `.shape` shows rows/columns.  \n",
    "- `.isna().sum()` counts missing values.  \n",
    "- `.duplicated().sum()` finds exact duplicated rows (good to know!).\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Selecting the target and features\n",
    "\n",
    "**Why:** We need to tell the model which column to predict (target) and which columns are inputs (features).\n",
    "\n",
    "```python\n",
    "TARGET_COL = \"diabetes_label\"\n",
    "feature_cols = [c for c in df.columns if c != TARGET_COL]\n",
    "X = df[feature_cols].copy()\n",
    "y = df[TARGET_COL].astype(int).copy()\n",
    "```\n",
    "\n",
    "- `TARGET_COL` is the column we want to predict.  \n",
    "- `feature_cols` is **all other columns** (we can prune later).  \n",
    "- `X` holds inputs; `y` holds labels.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Train / Validation / Test split (stratified)\n",
    "\n",
    "**Why:** We want to evaluate generalization on **unseen** data (test), and tune decisions (like **classification threshold**) on **validation** without touching the test set.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 80% Train+Val / 20% Test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 80% Train / 20% Val (of the 80% -> final 64/16/20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.20, stratify=y_train_val, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape} | Val: {X_val.shape} | Test: {X_test.shape}\")\n",
    "print(f\"Prevalence — Train: {y_train.mean():.3f} | Val: {y_val.mean():.3f} | Test: {y_test.mean():.3f}\")\n",
    "```\n",
    "\n",
    "- `stratify=y` preserves the positive rate in each split.  \n",
    "- `random_state=42` ensures reproducibility.  \n",
    "- We **don’t** look at the test set until the very end.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Feature Engineering (row-wise; no leakage)\n",
    "\n",
    "**Why:** Add medically meaningful predictors beyond raw values — improves model accuracy and interpretability.\n",
    "\n",
    "Below is the **exact function** used in your notebook — with **line-by-line explanation**:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ENGINEERED_COLS = [\n",
    "    \"ogtt_delta_1h\",\"ogtt_slope_0_1h\",\"ogtt_delta_2h\",\"ogtt_slope_0_2h\",\"ogtt_slope_1_2h\",\n",
    "    \"ogtt_auc_trap\",\"homa_ir\",\"fpg_prediabetes\",\"fpg_diabetes\",\"ogtt2h_prediabetes\",\n",
    "    \"ogtt2h_diabetes\",\"bmi_overweight\",\"bmi_obese\",\"insulin_cpep_ratio\",\n",
    "    \"bmi_x_fpg\",\"bmi_x_age\",\"age_sq\",\"bmi_sq\",\"glucose_fasting_sq\",\n",
    "    \"log1p_insulin_fasting\",\"log1p_c_peptide_fasting\",\"log1p_glucose_fasting\",\n",
    "]\n",
    "\n",
    "def _to_float(s):\n",
    "    # Convert a Series to float, turning bad strings into NaN safely\n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "\n",
    "def add_clinical_features(X: pd.DataFrame, drop_existing: bool = True) -> pd.DataFrame:\n",
    "    X = X.copy()  # don't modify caller's data in-place\n",
    "    \n",
    "    # If we already added features (you re-ran the cell), remove them to avoid duplicates\n",
    "    if drop_existing:\n",
    "        X.drop(columns=[c for c in ENGINEERED_COLS if c in X.columns], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # Short names for important columns (may be missing in some datasets)\n",
    "    g0  = X.get(\"glucose_fasting\")\n",
    "    g1  = X.get(\"ogtt_1h_glucose\")\n",
    "    g2  = X.get(\"ogtt_2h_glucose\")\n",
    "    bmi = X.get(\"bmi\")\n",
    "    ins = X.get(\"insulin_fasting\")\n",
    "    cpe = X.get(\"c_peptide_fasting\")\n",
    "    age = X.get(\"age\")\n",
    "\n",
    "    eps = 1e-6  # small constant to avoid division by zero\n",
    "\n",
    "    # --- OGTT dynamics (how glucose changes over the 2-hour test) ---\n",
    "    if g0 is not None and g1 is not None:\n",
    "        X[\"ogtt_delta_1h\"]   = (_to_float(g1) - _to_float(g0))       # rise from fasting to 1h\n",
    "        X[\"ogtt_slope_0_1h\"] = (_to_float(g1) - _to_float(g0))       # same as delta (1-hour interval)\n",
    "    if g0 is not None and g2 is not None:\n",
    "        X[\"ogtt_delta_2h\"]   = (_to_float(g2) - _to_float(g0))       # rise from fasting to 2h\n",
    "        X[\"ogtt_slope_0_2h\"] = (_to_float(g2) - _to_float(g0))       # same, 2-hour interval\n",
    "    if g1 is not None and g2 is not None:\n",
    "        X[\"ogtt_slope_1_2h\"] = (_to_float(g2) - _to_float(g1))       # change between 1h and 2h\n",
    "    \n",
    "    # Trapezoid AUC (area under glucose curve from 0h->1h->2h): a simple aggregate of OGTT\n",
    "    if (g0 is not None) and (g1 is not None) and (g2 is not None):\n",
    "        g0f, g1f, g2f = _to_float(g0), _to_float(g1), _to_float(g2)\n",
    "        X[\"ogtt_auc_trap\"] = 0.5*(g0f + g1f) + 0.5*(g1f + g2f)\n",
    "\n",
    "    # --- HOMA-IR: insulin resistance proxy (mg/dL version) ---\n",
    "    if (ins is not None) and (g0 is not None):\n",
    "        X[\"homa_ir\"] = _to_float(ins) * _to_float(g0) / 405.0\n",
    "\n",
    "    # --- Clinical flags at common guideline cut-offs ---\n",
    "    if g0 is not None:\n",
    "        g0f = _to_float(g0)\n",
    "        X[\"fpg_prediabetes\"] = ((g0f >= 100) & (g0f < 126)).astype(int)  # impaired fasting glucose\n",
    "        X[\"fpg_diabetes\"]    = (g0f >= 126).astype(int)                   # fasting diabetes threshold\n",
    "    if g2 is not None:\n",
    "        g2f = _to_float(g2)\n",
    "        X[\"ogtt2h_prediabetes\"] = ((g2f >= 140) & (g2f < 200)).astype(int) # IGT\n",
    "        X[\"ogtt2h_diabetes\"]    = (g2f >= 200).astype(int)                 # diabetes by 2h OGTT\n",
    "\n",
    "    if bmi is not None:\n",
    "        bmif = _to_float(bmi)\n",
    "        X[\"bmi_overweight\"] = ((bmif >= 25) & (bmif < 30)).astype(int)\n",
    "        X[\"bmi_obese\"]      = (bmif >= 30).astype(int)\n",
    "\n",
    "    # --- Ratios and interactions (capture non-linear relations simply) ---\n",
    "    if (ins is not None) and (cpe is not None):\n",
    "        X[\"insulin_cpep_ratio\"] = _to_float(ins) / (_to_float(cpe) + eps)  # secretion vs insulin proxy\n",
    "    if (bmi is not None) and (g0 is not None):\n",
    "        X[\"bmi_x_fpg\"] = _to_float(bmi) * _to_float(g0)\n",
    "    if (bmi is not None) and (age is not None):\n",
    "        X[\"bmi_x_age\"] = _to_float(bmi) * _to_float(age)\n",
    "\n",
    "    # --- Mild nonlinearity (squared terms) ---\n",
    "    if \"age\" in X.columns:\n",
    "        X[\"age_sq\"] = _to_float(X[\"age\"]) ** 2\n",
    "    if \"bmi\" in X.columns:\n",
    "        X[\"bmi_sq\"] = _to_float(X[\"bmi\"]) ** 2\n",
    "    if \"glucose_fasting\" in X.columns:\n",
    "        X[\"glucose_fasting_sq\"] = _to_float(X[\"glucose_fasting\"]) ** 2\n",
    "\n",
    "    # --- Log transforms for skewed variables (robust with clip to >=0) ---\n",
    "    if \"insulin_fasting\" in X.columns:\n",
    "        X[\"log1p_insulin_fasting\"] = np.log1p(_to_float(X[\"insulin_fasting\"]).clip(lower=0))\n",
    "    if \"c_peptide_fasting\" in X.columns:\n",
    "        X[\"log1p_c_peptide_fasting\"] = np.log1p(_to_float(X[\"c_peptide_fasting\"]).clip(lower=0))\n",
    "    if \"glucose_fasting\" in X.columns:\n",
    "        X[\"log1p_glucose_fasting\"] = np.log1p(_to_float(X[\"glucose_fasting\"]).clip(lower=0))\n",
    "\n",
    "    return X\n",
    "```\n",
    "\n",
    "**How to use it:** place after the split, then run:\n",
    "\n",
    "```python\n",
    "X_train = add_clinical_features(X_train)\n",
    "X_val   = add_clinical_features(X_val)\n",
    "X_test  = add_clinical_features(X_test)\n",
    "```\n",
    "\n",
    "This is **row-wise** (uses only the same person’s row), so it **cannot leak** information from validation/test into train.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Preprocessing (turn raw columns into model-ready numbers)\n",
    "\n",
    "**Why:** Models need clean numeric arrays.  \n",
    "We handle: missing values, scaling numeric columns, and encoding categorical columns.\n",
    "\n",
    "### 6.1 Column selection\n",
    "```python\n",
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "num_selector = make_column_selector(pattern=None, dtype_include=[\"number\"])\n",
    "cat_selector = make_column_selector(pattern=None, dtype_include=[\"object\", \"category\"])\n",
    "```\n",
    "\n",
    "- **Numeric columns**: get standardized (mean=0, std=1) for linear models.  \n",
    "- **Categorical columns**: get one‑hot encoded into 0/1 vectors.\n",
    "\n",
    "### 6.2 Build preprocessors\n",
    "\n",
    "- **Linear models** (Logistic Regression / Neural nets) benefit from scaled inputs.\n",
    "- **Tree models** (RF/XGBoost) do **not** require scaling; they just need missing values handled and categories encoded.\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "pre_linear = ColumnTransformer([\n",
    "    (\"num\", Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"sc\",  StandardScaler())\n",
    "    ]), num_selector),\n",
    "    (\"cat\", Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"oh\",  OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
    "    ]), cat_selector),\n",
    "])\n",
    "\n",
    "pre_tree = ColumnTransformer([\n",
    "    (\"num\", SimpleImputer(strategy=\"median\"), num_selector),\n",
    "    (\"cat\", Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"oh\",  OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]), cat_selector),\n",
    "])\n",
    "```\n",
    "\n",
    "- `SimpleImputer` handles missing values.  \n",
    "- `StandardScaler` scales numeric features for linear models.  \n",
    "- `OneHotEncoder` turns categories into binary columns.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Models and how they work (with code)\n",
    "\n",
    "### 7.1 Logistic Regression (linear model)\n",
    "\n",
    "**Intuition:** Finds a **straight line (or hyperplane)** in feature space that separates class 0 vs 1.  \n",
    "**Pros:** fast, interpretable. **Cons:** struggles with complex nonlinear boundaries.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_pipeline = Pipeline([\n",
    "    (\"pre\", pre_linear),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\", solver=\"lbfgs\"))\n",
    "])\n",
    "\n",
    "lr_pipeline.fit(X_train, y_train)    # learn weights\n",
    "proba_val = lr_pipeline.predict_proba(X_val)[:,1]  # predict probabilities\n",
    "```\n",
    "\n",
    "### 7.2 Random Forest (ensemble of decision trees)\n",
    "\n",
    "**Intuition:** Grows many decision trees on random subsets of rows/features, and averages their votes.  \n",
    "**Pros:** handles nonlinearity and interactions automatically. **Cons:** larger models, less interpretable than linear.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"pre\", pre_tree),\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        n_estimators=800, max_depth=10, min_samples_leaf=2,\n",
    "        class_weight=\"balanced_subsample\", n_jobs=-1, random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "proba_val = rf_pipeline.predict_proba(X_val)[:,1]\n",
    "```\n",
    "\n",
    "### 7.3 XGBoost (gradient-boosted trees)\n",
    "\n",
    "**Intuition:** Builds trees **sequentially**, each new tree corrects errors of the previous ones. Often top accuracy.  \n",
    "**Pros:** high accuracy, handles nonlinearity. **Cons:** many hyperparameters; can overfit if not tuned.\n",
    "\n",
    "```python\n",
    "# pip install xgboost --quiet   # if needed\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_pipeline = Pipeline([\n",
    "    (\"pre\", pre_tree),\n",
    "    (\"clf\", xgb.XGBClassifier(\n",
    "        n_estimators=800, max_depth=6, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        reg_lambda=1.0, random_state=42, eval_metric=\"auc\",\n",
    "        tree_method=\"hist\"  # set to \"gpu_hist\" if using GPU\n",
    "    ))\n",
    "])\n",
    "\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "proba_val = xgb_pipeline.predict_proba(X_val)[:,1]\n",
    "```\n",
    "\n",
    "> **GPU note:** On a CUDA GPU box, switch `tree_method=\"gpu_hist\"` (and optionally `predictor=\"gpu_predictor\"`).\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Threshold tuning on the Validation set (why and how)\n",
    "\n",
    "**Why:** Models output probabilities. You must pick a cutoff **threshold** to convert to 0/1.  \n",
    "Use validation to choose a threshold that balances **Precision** and **Recall** for your use case.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def pr_opt_threshold(y_true, proba):\n",
    "    p, r, t = precision_recall_curve(y_true, proba)\n",
    "    if len(t) <= 1:\n",
    "        return 0.5  # fallback\n",
    "    J = p + r - 1           # Youden-like index on PR\n",
    "    return float(t[np.argmax(J[:-1])])\n",
    "```\n",
    "\n",
    "- **Precision**: of predicted positives, how many are truly positive?  \n",
    "- **Recall**: of actual positives, how many did we catch?  \n",
    "- The **best threshold** is not always 0.5 — we pick it by validation!\n",
    "\n",
    "Apply it:\n",
    "```python\n",
    "thr = pr_opt_threshold(y_val, proba_val)\n",
    "pred_test = (xgb_pipeline.predict_proba(X_test)[:,1] >= thr).astype(int)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Metrics and what they mean\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss,\n",
    "    precision_score, recall_score\n",
    ")\n",
    "\n",
    "proba_test = xgb_pipeline.predict_proba(X_test)[:,1]\n",
    "pred_test  = (proba_test >= thr).astype(int)\n",
    "\n",
    "auroc = roc_auc_score(y_test, proba_test)         # ranking quality across thresholds\n",
    "ap    = average_precision_score(y_test, proba_test) # average precision (area under PR)\n",
    "brier = brier_score_loss(y_test, proba_test)      # calibration (lower is better)\n",
    "prec  = precision_score(y_test, pred_test)\n",
    "rec   = recall_score(y_test, pred_test)\n",
    "prev  = float(y_test.mean())\n",
    "lift  = (prec / prev) if prev > 0 else float(\"nan\")\n",
    "\n",
    "print(f\"AUC={auroc:.4f}  AP={ap:.4f}  Prec={prec:.3f}  Rec={rec:.3f}  Brier={brier:.4f}  Prev={prev:.3f}  Thr={thr:.3f}  Lift={lift:.2f}x\")\n",
    "```\n",
    "\n",
    "- **AUC (ROC)**: probability a random positive scores higher than a random negative.  \n",
    "- **AP (AUPRC)**: area under Precision-Recall; useful for imbalanced data.  \n",
    "- **Brier**: measures how close probabilities are to true outcomes (calibration).  \n",
    "- **Lift**: how much better your precision is than the base rate.\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Plots (what they show)\n",
    "\n",
    "- **Target distribution**: check class balance (imbalance affects metrics/threshold).  \n",
    "- **Correlation heatmap (numeric)**: linear relationships between features/target.  \n",
    "- **Age/BMI hist by label**: visualize separation.  \n",
    "- **PR and ROC curves**: show performance across thresholds.  \n",
    "- **Feature importance**: which features the model uses most.  \n",
    "- **Learning curve**: do we benefit from more data?  \n",
    "- **Validation curve**: how a hyperparameter affects CV score.\n",
    "\n",
    "> These plots help you explain *why* the model works and whether it’s overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## 11) K-fold Cross-Validation (stronger validation)\n",
    "\n",
    "**Why:** Instead of one split, rotate multiple folds to estimate generalization more robustly.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(lr_pipeline, X_train, y_train, cv=cv, scoring=\"roc_auc\", n_jobs=-1)\n",
    "print(\"5-fold CV ROC AUC (mean±std):\", scores.mean(), \"±\", scores.std())\n",
    "```\n",
    "\n",
    "> Use CV on the training set to compare models. Keep the **test set** unseen until final reporting.\n",
    "\n",
    "---\n",
    "\n",
    "## 12) Scaling to large data (e.g., 2 million rows)\n",
    "\n",
    "- Prefer **tree models** (RF/XGB) on CPUs with **n_jobs=-1** and moderate depth.  \n",
    "- For XGBoost, consider **GPU** (`tree_method=\"gpu_hist\"`) for big data if you have an NVIDIA GPU with CUDA.  \n",
    "- Use chunked reading (`pd.read_csv(..., chunksize=...)`) or consider **Dask/Polars** for bigger-than-memory.  \n",
    "- Avoid Python loops — keep operations vectorized in pandas/NumPy.\n",
    "\n",
    "---\n",
    "\n",
    "## 13) Connecting to a Data Warehouse (examples)\n",
    "\n",
    "> Keep credentials secure (env vars / secrets manager). The code below is commented so it won’t run accidentally.\n",
    "\n",
    "**BigQuery (pandas-gbq):**\n",
    "```python\n",
    "# pip install pandas-gbq google-cloud-bigquery --quiet\n",
    "# from google.oauth2 import service_account\n",
    "# import pandas as pd\n",
    "#\n",
    "# credentials = service_account.Credentials.from_service_account_file(\"gcp-key.json\")\n",
    "# sql = \"SELECT age, gender, bmi, glucose_fasting, insulin_fasting, c_peptide_fasting, ogtt_1h_glucose, ogtt_2h_glucose, diabetes_label FROM project.dataset.table\"\n",
    "# df = pd.read_gbq(sql, project_id=\"your-project\", credentials=credentials)\n",
    "```\n",
    "\n",
    "**Snowflake (SQLAlchemy):**\n",
    "```python\n",
    "# pip install snowflake-sqlalchemy --quiet\n",
    "# from sqlalchemy import create_engine\n",
    "# import pandas as pd\n",
    "#\n",
    "# engine = create_engine(\"snowflake://<user>:<password>@<account>/<db>/<schema>?warehouse=<wh>&role=<role>\")\n",
    "# df = pd.read_sql(\"SELECT ... FROM ...\", engine)\n",
    "```\n",
    "\n",
    "**SQL Server (ODBC / pymssql):**\n",
    "```python\n",
    "# pip install sqlalchemy pymssql --quiet\n",
    "# from sqlalchemy import create_engine\n",
    "# import pandas as pd\n",
    "#\n",
    "# engine = create_engine(\"mssql+pymssql://user:pwd@host:1433/DBNAME\")\n",
    "# df = pd.read_sql(\"SELECT ... FROM dbo.your_table\", engine)\n",
    "```\n",
    "\n",
    "**S3 Parquet (pyarrow):**\n",
    "```python\n",
    "# pip install s3fs pyarrow --quiet\n",
    "# import pandas as pd\n",
    "#\n",
    "# df = pd.read_parquet(\"s3://bucket/path/data.parquet\")\n",
    "```\n",
    "\n",
    "Once `df` is loaded from your warehouse, the rest of the pipeline is identical.\n",
    "\n",
    "---\n",
    "\n",
    "## 14) Using external GPUs\n",
    "\n",
    "- **XGBoost:** set `tree_method=\"gpu_hist\"` (fastest) and optionally `predictor=\"gpu_predictor\"`.  \n",
    "- **LightGBM:** use `device=\"gpu\"` (requires GPU-enabled build).  \n",
    "- **Neural nets (Keras/PyTorch):** will auto-detect GPU if installed with CUDA.\n",
    "\n",
    "**XGBoost GPU example:**\n",
    "```python\n",
    "xgb.XGBClassifier(\n",
    "    tree_method=\"gpu_hist\",\n",
    "    predictor=\"gpu_predictor\",\n",
    "    n_estimators=800, max_depth=6, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0, eval_metric=\"auc\",\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 15) Common pitfalls & fixes\n",
    "\n",
    "- **Pandas `.clip(min=0)` error** → Use `.clip(lower=0)` (pandas uses lower/upper).  \n",
    "- **OneHotEncoder + pandas output warning** → Set `sparse=False` for DataFrame output, or rely on default NumPy array output.  \n",
    "- **BrokenProcessPool in Joblib** → Usually environment conflict; set `n_jobs=1` or restart kernel/venv.  \n",
    "- **Perfect validation AUC = 1.0** → Check for **leakage** or **duplicates across splits**.  \n",
    "- **Mismatched units (mmol/L vs mg/dL)** → Convert units or adapt formulas (e.g., HOMA-IR).\n",
    "\n",
    "---\n",
    "\n",
    "## 16) How to present results (talk track)\n",
    "\n",
    "1) **Problem & Data** — what’s being predicted, what features, how much data.  \n",
    "2) **Quality checks** — missing values, duplicates, outliers.  \n",
    "3) **Split strategy** — Train/Val/Test, stratified.  \n",
    "4) **Feature engineering** — medical rationale (HOMA-IR, OGTT deltas).  \n",
    "5) **Models compared** — LR (interpretable), RF (nonlinear), XGB (state-of-the-art).  \n",
    "6) **Threshold selection** — tuned on validation to balance precision/recall.  \n",
    "7) **Final metrics on Test** — AUC/AP/Precision/Recall/Brier/Lift.  \n",
    "8) **Plots** — PR/ROC, feature importances, learning/validation curves.  \n",
    "9) **Scalability & Ops** — GPU option, warehouse integration, versioning, reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## 17) Repro: quick run order\n",
    "\n",
    "1) Load CSV → build `df`  \n",
    "2) Choose `TARGET_COL` / `feature_cols`  \n",
    "3) Train/Val/Test split (stratified)  \n",
    "4) **Feature Engineering** (this guide’s function)  \n",
    "5) Define preprocessors (`pre_linear`, `pre_tree`)  \n",
    "6) Train models (LR, RF, XGB)  \n",
    "7) Tune threshold on **Validation**  \n",
    "8) Evaluate on **Test**  \n",
    "9) Generate plots  \n",
    "10) (Optional) K-fold CV, calibration, GPU variants, warehouse connectors\n",
    "\n",
    "---\n",
    "\n",
    "*You now have both the “how” and the “why” for each step. If you want, I can also generate a slide deck version (PowerPoint) with the same content and charts.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbc3b35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
